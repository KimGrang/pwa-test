import { useState, useCallback, useRef } from 'react';
import { ChatMessage } from '../types/pwa';

// wllama íƒ€ìž… ì •ì˜
interface Wllama {
  loadModelFromUrl: (
    url: string,
    options: { progressCallback?: (progress: { loaded: number; total: number }) => void }
  ) => Promise<void>;
  createCompletion: (
    prompt: string,
    options: { nPredict: number; sampling: { temp: number; top_k: number; top_p: number; repeat_penalty: number } }
  ) => Promise<string>;
}

export interface WllamaHookConfig {
  temperature?: number;
  topP?: number;
  maxTokens?: number;
  repetitionPenalty?: number;
}

export function useWllama(config: WllamaHookConfig = {}) {
  const [isLoaded, setIsLoaded] = useState(false);
  const [progress, setProgress] = useState(0);
  const [isLoading, setIsLoading] = useState(false);
  const [modelInfo, setModelInfo] = useState<string>('');
  const wllamaRef = useRef<Wllama | null>(null);

  // ê¸°ë³¸ ì„¤ì •
  const defaultConfig: WllamaHookConfig = {
    temperature: 0.7,
    topP: 0.9,
    maxTokens: 512,
    repetitionPenalty: 1.1,
    ...config,
  };

  // wllama ì´ˆê¸°í™”
  const initializeWllama = useCallback(async () => {
    if (wllamaRef.current) return wllamaRef.current;

    try {
      console.log('ðŸš€ wllama ì´ˆê¸°í™” ì‹œìž‘...');

      // ë™ì  importë¡œ wllama ë¡œë“œ
      const { Wllama, LoggerWithoutDebug } = await import('@wllama/wllama');

      // wllama ì„¤ì • ê²½ë¡œ (CDNì—ì„œ wasm íŒŒì¼ ë¡œë“œ)
      const CONFIG_PATHS = {
        'single-thread/wllama.wasm': 'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.3.4/esm/single-thread/wllama.wasm',
        'multi-thread/wllama.wasm': 'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.3.4/esm/multi-thread/wllama.wasm',
      };

      // wllama ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (LoggerWithoutDebugë¡œ ë””ë²„ê·¸ ë©”ì‹œì§€ ì–µì œ)
      const wllama = new Wllama(CONFIG_PATHS, {
        logger: LoggerWithoutDebug,
      });

      wllamaRef.current = wllama;

      console.log('âœ… wllama ì´ˆê¸°í™” ì™„ë£Œ');
      return wllama;
    } catch (error) {
      console.error('âŒ wllama ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      throw error;
    }
  }, []);

  // ëª¨ë¸ ë¡œë”©
  const loadModel = useCallback(
    async (modelPath: string) => {
      try {
        setIsLoading(true);
        setProgress(0);
        setModelInfo(`ëª¨ë¸ ë¡œë”© ì¤‘: ${modelPath}`);
        console.log('ðŸš€ wllama ëª¨ë¸ ë¡œë”© ì‹œìž‘...', modelPath);

        const wllama = await initializeWllama();

        // ì§„í–‰ë¥  ì½œë°±
        const progressCallback = ({ loaded, total }: { loaded: number; total: number }) => {
          const progressPercent = Math.round((loaded / total) * 100);
          setProgress(progressPercent);
          console.log(`ðŸ“Š ë¡œë”© ì§„í–‰ë¥ : ${progressPercent}%`);
        };

        console.log('ðŸ“ ëª¨ë¸ ë¡œë”© ì‹œìž‘:', modelPath);

        // ë¡œì»¬ íŒŒì¼ URLë¡œ ëª¨ë¸ ë¡œë”©
        const modelUrl = new URL(modelPath, window.location.origin).href;
        console.log('ðŸ”— ëª¨ë¸ URL:', modelUrl);

        await wllama.loadModelFromUrl(modelUrl, {
          progressCallback,
        });

        setIsLoaded(true);
        setProgress(100);
        setModelInfo(`âœ… ${modelPath} ëª¨ë¸ ë¡œë”© ì™„ë£Œ`);
        console.log('âœ… wllama ëª¨ë¸ ë¡œë”© ì™„ë£Œ');
      } catch (error) {
        console.error('âŒ wllama ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨:', error);
        const errorMessage = error instanceof Error ? error.message : String(error);
        setModelInfo(`âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: ${errorMessage}`);
        throw error;
      } finally {
        setIsLoading(false);
      }
    },
    [initializeWllama]
  );

  // ì±„íŒ… ì™„ë£Œ
  const chatCompletion = useCallback(
    async (messages: ChatMessage[], onStream?: (text: string) => void) => {
      if (!isLoaded || !wllamaRef.current) {
        throw new Error('ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.');
      }

      try {
        setIsLoading(true);
        console.log('ðŸš€ wllama ì±„íŒ… ì‹œìž‘...');

        // ë§ˆì§€ë§‰ ì‚¬ìš©ìž ë©”ì‹œì§€ ì¶”ì¶œ
        const lastUserMessage = messages.filter((msg) => msg.role === 'user').pop()?.content || '';

        console.log('ðŸ“ ì‚¬ìš©ìž ë©”ì‹œì§€:', lastUserMessage);

        // ì±„íŒ… ì™„ë£Œ ì˜µì…˜
        const completionOptions = {
          nPredict: defaultConfig.maxTokens || 512,
          sampling: {
            temp: defaultConfig.temperature || 0.7,
            top_k: 40,
            top_p: defaultConfig.topP || 0.9,
            repeat_penalty: defaultConfig.repetitionPenalty || 1.1,
          },
        };

        console.log('âš™ï¸ ì±„íŒ… ì˜µì…˜:', completionOptions);

        // ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
        const outputText = await wllamaRef.current.createCompletion(lastUserMessage, completionOptions);

        console.log('âœ… ì±„íŒ… ì™„ë£Œ, ì „ì²´ ì‘ë‹µ:', outputText);

        // ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ (wllamaëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ)
        if (onStream) {
          const words = outputText.split(' ');
          for (const word of words) {
            onStream(word + ' ');
            await new Promise((resolve) => setTimeout(resolve, 50)); // 50ms ì§€ì—°
          }
        }

        return outputText;
      } catch (error) {
        console.error('âŒ wllama ì±„íŒ… ì™„ë£Œ ì‹¤íŒ¨:', error);
        throw error;
      } finally {
        setIsLoading(false);
      }
    },
    [isLoaded, defaultConfig]
  );

  // ëª¨ë¸ ì–¸ë¡œë“œ
  const unloadModel = useCallback(() => {
    if (wllamaRef.current) {
      // wllamaëŠ” ëª¨ë¸ ì–¸ë¡œë“œ ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì°¸ì¡°ë§Œ ì œê±°
      wllamaRef.current = null;
    }
    setIsLoaded(false);
    setProgress(0);
    setModelInfo('');
  }, []);

  // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (ë¡œì»¬ íŒŒì¼ë“¤)
  const getAvailableModels = useCallback(() => {
    return [
      '/models/euro_gguf.gguf',
      // ì¶”ê°€ ë¡œì»¬ ëª¨ë¸ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
    ];
  }, []);

  // ë¡œì»¬ ëª¨ë¸ ëª©ë¡ (wllamaëŠ” ë¡œì»¬ íŒŒì¼ì„ ì§ì ‘ ì§€ì›)
  const getLocalModels = useCallback(() => {
    return ['/models/euro_gguf.gguf'];
  }, []);

  return {
    loadModel,
    chatCompletion,
    unloadModel,
    isLoaded,
    isLoading,
    progress,
    modelInfo,
    getAvailableModels,
    getLocalModels,
  };
}
